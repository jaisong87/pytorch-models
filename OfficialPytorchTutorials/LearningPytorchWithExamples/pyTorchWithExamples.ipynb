{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement NN with training in numpy\n",
    "In this section we implement a 2-layer neural network in numpy\n",
    "and write code to compute gradients and train it. In later sections\n",
    "we will substitute this with pyTorch framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch#10 = 3846733.26435\n",
      "Loss after epoch#20 = 295808.615995\n",
      "Loss after epoch#30 = 92206.8380658\n",
      "Loss after epoch#40 = 35240.335839\n",
      "Loss after epoch#50 = 14941.9370407\n",
      "Loss after epoch#60 = 6800.10974393\n",
      "Loss after epoch#70 = 3252.48935337\n",
      "Loss after epoch#80 = 1618.7259482\n",
      "Loss after epoch#90 = 833.413745886\n",
      "Loss after epoch#100 = 441.501260191\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if epoch % 10 == 0:\n",
    "        print (\"Loss after epoch#{} = {}\".format(epoch, loss))\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw NN code using torch\n",
    "In this section, we write the same code but using pytorch. torch\n",
    "tensors can easily be instantiated on cpu / gpu for speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch#10 = 1915199.0\n",
      "Loss after epoch#20 = 233240.65625\n",
      "Loss after epoch#30 = 73698.953125\n",
      "Loss after epoch#40 = 27723.0371094\n",
      "Loss after epoch#50 = 11382.8535156\n",
      "Loss after epoch#60 = 4953.1875\n",
      "Loss after epoch#70 = 2240.3671875\n",
      "Loss after epoch#80 = 1042.73400879\n",
      "Loss after epoch#90 = 496.761627197\n",
      "Loss after epoch#100 = 242.443161011\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for epoch in range(1, 101):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if epoch % 10 == 0:\n",
    "        print (\"Loss after epoch#{} = {}\".format(epoch, loss))\n",
    "\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGrad based training\n",
    "Train a model using pyTorch but using autograd to avoid manual differentiation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch#10 = 2626682.5\n",
      "Loss after epoch#20 = 284237.65625\n",
      "Loss after epoch#30 = 96463.0859375\n",
      "Loss after epoch#40 = 39380.3554688\n",
      "Loss after epoch#50 = 17924.28125\n",
      "Loss after epoch#60 = 8750.51855469\n",
      "Loss after epoch#70 = 4483.93017578\n",
      "Loss after epoch#80 = 2377.48120117\n",
      "Loss after epoch#90 = 1295.20019531\n",
      "Loss after epoch#100 = 721.10559082\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for epoch in range(1, 101):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    # Compute and print loss\n",
    "    if epoch % 10 == 0:\n",
    "        print (\"Loss after epoch#{} = {}\".format(epoch, loss.item()))\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement custom Function with AutoGrad\n",
    "In this section we implement our version of relu called myRelu\n",
    "and we specify forward pass as well as backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model using Custom Function\n",
    "Now we train a model using the custom implementation of RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch#10 = 1696471.625\n",
      "Loss after epoch#20 = 263291.0\n",
      "Loss after epoch#30 = 86070.578125\n",
      "Loss after epoch#40 = 33632.5039062\n",
      "Loss after epoch#50 = 14504.90625\n",
      "Loss after epoch#60 = 6679.05126953\n",
      "Loss after epoch#70 = 3211.20874023\n",
      "Loss after epoch#80 = 1596.90136719\n",
      "Loss after epoch#90 = 815.346496582\n",
      "Loss after epoch#100 = 425.159057617\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for epoch in range(1, 101):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    # Compute and print loss\n",
    "    if epoch % 10 == 0:\n",
    "        print (\"Loss after epoch#{} = {}\".format(epoch, loss.item()))\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pytorch nn package to define NN\n",
    "In this section we use pytorch.nn package but still perform manual backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch#10 = 330.044769287\n",
      "Loss after epoch#20 = 193.611541748\n",
      "Loss after epoch#30 = 110.871696472\n",
      "Loss after epoch#40 = 61.8626441956\n",
      "Loss after epoch#50 = 34.657333374\n",
      "Loss after epoch#60 = 19.8393688202\n",
      "Loss after epoch#70 = 11.6505479813\n",
      "Loss after epoch#80 = 7.00678491592\n",
      "Loss after epoch#90 = 4.30923891068\n",
      "Loss after epoch#100 = 2.70535635948\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for epoch in range(1, 101):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if epoch % 10 == 0:\n",
    "        print (\"Loss after epoch#{} = {}\".format(epoch, loss.item()))\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use pytorch.optim package\n",
    "We use AdamOptimizer from the optim package to train our simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch#10 = 581.626831055\n",
      "Loss after epoch#20 = 463.609008789\n",
      "Loss after epoch#30 = 373.10748291\n",
      "Loss after epoch#40 = 300.95022583\n",
      "Loss after epoch#50 = 241.537628174\n",
      "Loss after epoch#60 = 192.123901367\n",
      "Loss after epoch#70 = 150.791213989\n",
      "Loss after epoch#80 = 116.273269653\n",
      "Loss after epoch#90 = 87.9394683838\n",
      "Loss after epoch#100 = 65.2383651733\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(1, 101):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if epoch % 10 == 0:\n",
    "        print (\"Loss after epoch#{} = {}\".format(epoch, loss.item()))\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom model using nn.Module\n",
    "In this section we define a custom 2 layer NN and use this class to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch#10 = 413.988739014\n",
      "Loss after epoch#20 = 240.924194336\n",
      "Loss after epoch#30 = 135.251693726\n",
      "Loss after epoch#40 = 73.4742279053\n",
      "Loss after epoch#50 = 39.767250061\n",
      "Loss after epoch#60 = 21.9052352905\n",
      "Loss after epoch#70 = 12.3572177887\n",
      "Loss after epoch#80 = 7.16591978073\n",
      "Loss after epoch#90 = 4.27396297455\n",
      "Loss after epoch#100 = 2.6149096489\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for epoch in range(1, 101):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if epoch % 10 == 0:\n",
    "        print (\"Loss after epoch#{} = {}\".format(epoch, loss.item()))\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DynamicNet - An unusual model\n",
    "To illustrate dynamic computation graphs, we implement a network where forward propogation will reuse\n",
    "same weights for oneof the hidden layer that is evaluated 0, 1, 2 or 3 times based on a random variable.\n",
    "This model is defined to illustrate dynamic computation graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch#10 = 236.231506348\n",
      "Loss after epoch#20 = 349.229034424\n",
      "Loss after epoch#30 = 159.830245972\n",
      "Loss after epoch#40 = 126.480506897\n",
      "Loss after epoch#50 = 171.989425659\n",
      "Loss after epoch#60 = 20.1855106354\n",
      "Loss after epoch#70 = 25.2583751678\n",
      "Loss after epoch#80 = 41.6708030701\n",
      "Loss after epoch#90 = 14.4199466705\n",
      "Loss after epoch#100 = 30.158908844\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for epoch in range(1, 101):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if epoch % 10 == 0:\n",
    "        print (\"Loss after epoch#{} = {}\".format(epoch, loss.item()))\n",
    "\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, tensor([[-2.7946e-02,  1.3537e-03, -4.6445e-02,  ...,  8.6383e-03,\n",
      "         -1.4318e-02, -3.9209e-02],\n",
      "        [ 2.5804e-03,  1.2038e-02, -7.3610e-03,  ..., -1.4908e-02,\n",
      "         -2.9497e-02,  1.1369e-03],\n",
      "        [-4.1324e-03, -2.1633e-03, -2.8950e-02,  ..., -5.5250e-03,\n",
      "          1.2308e-02,  2.9307e-02],\n",
      "        ...,\n",
      "        [-2.1104e-02,  2.9639e-02,  1.4718e-02,  ..., -2.0050e-02,\n",
      "         -3.0024e-02, -1.1952e-02],\n",
      "        [-3.6979e-05,  1.2809e-02,  5.3615e-03,  ..., -2.5803e-02,\n",
      "         -2.7878e-02, -1.4689e-02],\n",
      "        [ 1.2043e-02, -7.2605e-04,  2.3104e-02,  ...,  1.9859e-02,\n",
      "          1.3871e-02, -1.4269e-02]]))\n",
      "(None, tensor([-0.0130,  0.0148, -0.0072, -0.0361,  0.0065, -0.0196,  0.0245,  0.0007,\n",
      "         0.0117, -0.0269,  0.0207, -0.0015,  0.0116,  0.0132, -0.0104, -0.0074,\n",
      "        -0.0221, -0.0253, -0.0350,  0.0249,  0.0035, -0.0145,  0.0139,  0.0209,\n",
      "        -0.0150, -0.0065, -0.0078, -0.0105,  0.0310, -0.0232, -0.0065,  0.0135,\n",
      "        -0.0108, -0.0028, -0.0142,  0.0130,  0.0292, -0.0222,  0.0249, -0.0100,\n",
      "        -0.0306, -0.0316,  0.0046,  0.0061,  0.0115, -0.0288, -0.0189, -0.0026,\n",
      "        -0.0080, -0.0315, -0.0305,  0.0211,  0.0076, -0.0099, -0.0143,  0.0258,\n",
      "        -0.0184,  0.0040, -0.0031, -0.0152,  0.0075,  0.0046,  0.0067,  0.0052,\n",
      "         0.0122, -0.0315, -0.0144,  0.0167, -0.0204,  0.0283,  0.0328, -0.0202,\n",
      "        -0.0329,  0.0304,  0.0004, -0.0211, -0.0066, -0.0150, -0.0033, -0.0198,\n",
      "        -0.0093, -0.0185, -0.0130,  0.0065,  0.0216, -0.0162, -0.0166, -0.0034,\n",
      "        -0.0212, -0.0011,  0.0053, -0.0157, -0.0162,  0.0041, -0.0155, -0.0083,\n",
      "        -0.0023,  0.0089,  0.0133,  0.0234]))\n",
      "(None, tensor([[ 1.8389e-01,  8.3278e-02, -1.6332e-01,  ...,  1.3185e-01,\n",
      "         -2.6554e-02, -1.0257e-01],\n",
      "        [ 3.3241e-02,  1.1216e-01, -5.0540e-02,  ..., -5.6945e-02,\n",
      "          2.2859e-04,  4.7765e-02],\n",
      "        [-1.0902e-01,  8.8617e-02,  2.6236e-01,  ...,  2.0111e-02,\n",
      "          1.2752e-02,  3.8614e-02],\n",
      "        ...,\n",
      "        [ 9.5669e-02, -1.9483e-02,  7.3827e-02,  ...,  3.7116e-02,\n",
      "          8.0481e-02, -7.4088e-02],\n",
      "        [-6.9520e-02, -7.1724e-02,  6.5528e-02,  ..., -5.3994e-02,\n",
      "          1.1003e-01,  8.2989e-02],\n",
      "        [-1.2307e-01, -2.4027e-03,  1.0493e-01,  ..., -1.9823e-02,\n",
      "         -2.3060e-03, -1.0805e-02]]))\n",
      "(None, tensor([ 0.0958, -0.0134, -0.0004,  0.0817, -0.0183,  0.0438,  0.0345,  0.0898,\n",
      "        -0.0455, -0.1095, -0.0676,  0.0046, -0.0009,  0.1448, -0.0651,  0.0704,\n",
      "        -0.0296, -0.0278, -0.0549,  0.0457, -0.0706, -0.0438, -0.0284,  0.0494,\n",
      "         0.0536,  0.0817,  0.0827, -0.0422,  0.0989, -0.0183,  0.0721,  0.0177,\n",
      "         0.0179,  0.1024,  0.0698, -0.0126,  0.0208, -0.0578, -0.0647,  0.0504,\n",
      "         0.0245, -0.1060,  0.0497, -0.0372, -0.0416,  0.0026,  0.0908, -0.0850,\n",
      "         0.0416, -0.0831, -0.0320,  0.0095, -0.0614, -0.0237,  0.0659,  0.0148,\n",
      "        -0.0366,  0.1206, -0.0957, -0.0131,  0.0669,  0.0792, -0.0777,  0.0180,\n",
      "         0.1646, -0.0329, -0.0619,  0.0198, -0.0939,  0.0256, -0.0611,  0.1197,\n",
      "         0.0526, -0.0174,  0.0328, -0.0994, -0.1216,  0.0238, -0.0844, -0.0627,\n",
      "        -0.0188, -0.1029, -0.0646,  0.0221,  0.0788, -0.0809,  0.0158, -0.0390,\n",
      "         0.0870,  0.0730,  0.0715,  0.0628, -0.0599, -0.0475, -0.0159,  0.0562,\n",
      "        -0.0286, -0.0271, -0.0400,  0.0414]))\n",
      "(None, tensor([[-8.9280e-02,  1.1675e-01,  1.4541e-01, -4.5537e-02, -1.0921e-01,\n",
      "          1.8996e-01, -1.3683e-01, -1.1558e-01, -2.1865e-02,  1.7546e-01,\n",
      "         -5.0247e-02,  5.5906e-03, -5.4235e-02,  1.1939e-01,  7.1698e-03,\n",
      "         -7.3503e-02,  1.0797e-01,  1.5261e-03, -3.0201e-02, -4.9847e-02,\n",
      "          4.9103e-02,  4.5549e-02, -3.1119e-02,  9.3446e-02, -1.6241e-02,\n",
      "         -6.4983e-02,  5.2777e-03,  7.6436e-02,  7.5681e-02, -3.8096e-02,\n",
      "         -5.7043e-02,  6.0307e-04, -1.1835e-01,  1.0337e-01,  5.3175e-02,\n",
      "          1.0683e-02,  1.0539e-01, -4.2134e-03,  1.2209e-01,  9.8505e-03,\n",
      "         -2.1990e-02,  1.1087e-01, -8.0928e-02, -7.8125e-02,  9.3022e-03,\n",
      "         -1.0777e-01,  1.1241e-01,  1.7052e-02, -5.1024e-02,  7.4661e-02,\n",
      "          1.7749e-02, -1.4477e-01,  7.3124e-03, -5.3098e-03,  1.0980e-01,\n",
      "          7.5522e-02, -1.0271e-01,  1.5588e-01,  1.0604e-02,  2.2543e-02,\n",
      "         -1.5634e-01,  9.5191e-02, -1.9247e-02,  9.7512e-02, -4.5151e-02,\n",
      "          1.5580e-01,  1.8524e-02,  4.1403e-02, -6.5816e-02, -6.0751e-02,\n",
      "         -1.1494e-01, -1.2091e-02, -1.5487e-02, -2.0879e-02,  1.6422e-01,\n",
      "          1.2425e-01, -6.7212e-02, -1.1865e-01,  1.6781e-02, -5.4885e-02,\n",
      "          3.4790e-02, -2.5715e-02, -3.4689e-02, -7.9464e-02, -1.1955e-01,\n",
      "         -2.9164e-02, -7.5622e-02, -1.2783e-02, -7.7237e-02,  1.3943e-02,\n",
      "         -7.6290e-02, -6.0542e-03,  9.3337e-02, -2.7773e-02, -4.0841e-02,\n",
      "          2.0046e-02, -3.2178e-02,  5.7107e-03,  1.4470e-02,  9.7115e-02],\n",
      "        [ 5.6919e-02,  7.7090e-02, -7.6894e-02, -1.6013e-01,  8.7022e-02,\n",
      "          1.0871e-03, -7.2931e-02,  3.6920e-02,  1.1409e-01,  7.1759e-02,\n",
      "         -8.8461e-02, -5.7757e-02, -7.5311e-02, -1.2702e-01, -4.1340e-02,\n",
      "          1.0033e-01,  7.9041e-02, -7.9475e-02, -5.2742e-02, -9.6126e-02,\n",
      "         -5.5910e-02,  3.3646e-02, -9.7939e-02, -5.2884e-02,  7.9968e-02,\n",
      "         -1.2832e-01,  8.3768e-02,  5.2392e-02,  8.9400e-02,  3.8971e-02,\n",
      "          4.6699e-02, -1.4404e-01, -1.0517e-01, -7.4600e-02,  1.4501e-02,\n",
      "          1.3881e-01, -1.1588e-01,  9.5014e-02, -6.8338e-02,  1.8933e-02,\n",
      "         -1.2262e-01, -9.0248e-02,  8.0599e-02,  7.0780e-02,  3.3572e-02,\n",
      "         -1.6157e-02,  1.0380e-01, -1.5546e-01,  7.3725e-02, -1.2902e-01,\n",
      "         -7.7943e-02, -6.9884e-02,  2.8722e-02,  3.9463e-03,  1.7723e-01,\n",
      "         -7.8420e-02, -7.1386e-03,  2.6501e-02, -5.9954e-02, -8.8567e-02,\n",
      "         -6.0978e-02,  4.0979e-02, -7.4799e-02, -2.3221e-01,  1.2846e-01,\n",
      "          1.2639e-01,  7.1795e-02,  5.9444e-02, -4.4365e-02, -3.9328e-02,\n",
      "         -5.8266e-02, -1.0048e-01,  1.7644e-02,  7.2157e-02, -1.3783e-02,\n",
      "         -8.7814e-02,  5.3539e-03,  6.3145e-02,  1.6685e-02, -5.1487e-02,\n",
      "          1.3732e-01, -3.7360e-02,  7.2670e-02,  2.2964e-01, -8.0954e-02,\n",
      "         -2.2185e-02,  1.4632e-01,  5.6302e-02,  5.2955e-02, -4.8465e-02,\n",
      "          6.1671e-02, -1.6897e-01,  4.9811e-02, -1.7454e-03, -1.5776e-01,\n",
      "         -6.6356e-02,  2.1931e-02,  1.0590e-01,  8.6668e-02, -3.1298e-02],\n",
      "        [ 2.3177e-01,  3.1140e-02, -1.3936e-02,  1.8671e-02, -1.7654e-02,\n",
      "         -5.6213e-02, -7.3771e-02, -8.5810e-03,  6.7937e-02, -9.7306e-02,\n",
      "          8.6712e-02,  3.8825e-02,  7.6185e-02,  9.2064e-02, -8.8438e-02,\n",
      "         -1.7543e-02, -1.4031e-02, -3.8429e-02, -1.2262e-01, -3.9454e-02,\n",
      "         -6.5084e-02,  3.2626e-02,  1.2310e-01,  1.2932e-02, -5.8529e-02,\n",
      "         -3.4132e-02, -1.0013e-02, -8.2600e-02,  6.1742e-02, -4.6316e-02,\n",
      "         -5.8633e-02, -8.7722e-02, -1.2147e-01, -6.0110e-02, -7.9749e-02,\n",
      "         -1.4408e-01,  1.5161e-01, -1.2573e-01,  4.8490e-02, -3.5272e-02,\n",
      "         -1.0810e-01, -3.0689e-02,  4.9582e-02, -4.4480e-02,  4.2526e-02,\n",
      "          7.7585e-02,  2.5449e-02, -2.9627e-02, -2.8234e-02,  2.7028e-02,\n",
      "         -5.5813e-02, -1.0869e-01,  4.2134e-02,  2.3329e-01,  8.3311e-03,\n",
      "         -8.7763e-02, -1.2937e-01,  1.0371e-01, -1.4822e-01,  6.1830e-02,\n",
      "         -9.7086e-02,  2.1221e-02, -8.1650e-02,  4.9030e-02, -7.7916e-03,\n",
      "         -1.3680e-01,  5.8347e-03,  8.5356e-02,  7.2367e-02, -1.4998e-01,\n",
      "          4.7356e-02, -2.0246e-02,  5.2664e-02, -5.3078e-02,  8.9123e-02,\n",
      "         -2.0386e-02,  6.9960e-02,  5.3586e-02, -3.1938e-03, -4.1329e-02,\n",
      "         -4.9552e-02, -5.7888e-03, -1.1161e-02, -7.8217e-02,  1.4849e-01,\n",
      "         -7.0205e-02,  1.0923e-01,  7.1725e-02,  3.4018e-02,  1.7583e-01,\n",
      "          1.0285e-02,  1.8483e-01, -5.9423e-02, -8.8352e-02, -1.0052e-01,\n",
      "          3.5786e-02,  1.4725e-01,  5.1680e-02, -9.1345e-02, -5.5889e-02],\n",
      "        [-7.9780e-02,  3.4178e-02,  3.1330e-02, -3.5840e-02,  5.4966e-02,\n",
      "         -2.0259e-03,  6.2257e-02, -1.4628e-02, -3.4588e-02, -2.0894e-01,\n",
      "         -5.3693e-02, -1.1275e-01,  2.3617e-02,  8.8263e-02,  2.7500e-02,\n",
      "          1.0269e-01, -1.4763e-01,  8.9557e-02, -1.6813e-01,  4.2534e-02,\n",
      "         -4.9432e-02,  6.2111e-02, -3.3274e-02, -2.6199e-02, -8.9900e-02,\n",
      "         -1.0366e-01,  8.6206e-02,  1.0936e-01,  4.8673e-02, -1.7316e-01,\n",
      "          5.7083e-02, -1.8176e-01, -6.9636e-02, -9.9150e-03,  1.9999e-01,\n",
      "          7.0400e-02, -6.0589e-02,  1.1763e-01, -1.5341e-01,  1.4244e-01,\n",
      "          4.4211e-02, -7.3193e-02, -8.3221e-02, -1.8581e-02,  6.9721e-02,\n",
      "         -1.2479e-01,  1.2832e-01, -6.8336e-02, -7.6461e-03,  3.4145e-02,\n",
      "         -3.4607e-02, -2.9138e-02, -7.2592e-02,  5.0456e-02,  1.1686e-02,\n",
      "          1.3962e-01,  9.4514e-02,  1.2162e-01, -1.0389e-01,  1.0068e-01,\n",
      "          3.1265e-02, -1.0114e-01,  6.8982e-02,  7.1324e-02, -1.0548e-01,\n",
      "         -1.1220e-02,  4.5962e-02,  1.0515e-02,  4.5791e-02,  5.0274e-02,\n",
      "          1.4971e-01, -1.7721e-01,  5.9684e-02,  1.6177e-01,  4.4043e-02,\n",
      "         -8.1571e-02,  1.4758e-02, -4.9952e-02,  9.6280e-02,  3.8203e-02,\n",
      "          7.0090e-02, -1.4361e-03,  7.8853e-03, -2.6621e-04,  1.4192e-01,\n",
      "         -3.6578e-03, -8.0821e-02, -2.6564e-02, -9.8768e-02,  1.3581e-01,\n",
      "         -7.5914e-02, -5.6236e-02, -8.3883e-02,  1.1488e-01,  8.2474e-02,\n",
      "          2.6763e-02,  1.6119e-02,  5.9266e-02, -9.5358e-03, -6.6544e-02],\n",
      "        [-9.3356e-03,  1.0731e-01,  5.7685e-02, -4.6210e-02,  6.6829e-04,\n",
      "         -1.7768e-02,  4.8066e-02, -1.1423e-02, -3.3707e-02,  1.4841e-01,\n",
      "          7.8368e-02,  8.0667e-02, -7.9900e-02, -1.1274e-01, -3.6147e-03,\n",
      "         -1.0933e-01, -1.5350e-01, -1.3069e-01, -2.5110e-02,  1.1177e-02,\n",
      "          7.4428e-03,  2.4020e-03,  2.0640e-01,  7.9713e-02,  7.6187e-03,\n",
      "          5.1282e-02,  2.1033e-02, -1.0615e-01, -3.6233e-02, -8.7636e-02,\n",
      "          1.3212e-02,  2.9418e-02,  5.2130e-02, -8.7284e-02,  4.3631e-02,\n",
      "         -1.0495e-02,  4.1725e-02, -6.8828e-02,  7.0039e-02,  8.2539e-02,\n",
      "         -1.1045e-01, -2.6990e-02,  1.0317e-01,  8.7972e-02,  5.1927e-02,\n",
      "         -1.8653e-01, -2.5133e-02, -1.5906e-02,  9.5286e-02,  1.2529e-02,\n",
      "          1.5886e-01, -6.9741e-02,  2.1926e-02,  1.2755e-01, -1.2186e-01,\n",
      "          1.0943e-01, -7.7043e-02, -9.9003e-02, -3.2591e-02, -1.3781e-02,\n",
      "          8.6789e-02, -7.6110e-02,  1.6056e-02, -2.9459e-02,  9.9943e-02,\n",
      "         -1.4445e-01, -2.1146e-02,  1.4513e-03,  1.0307e-01,  4.5317e-02,\n",
      "         -5.7229e-02,  1.0205e-01, -2.3049e-02,  6.5607e-02, -1.0580e-01,\n",
      "         -3.5195e-02,  6.1847e-02, -2.9138e-02,  5.8655e-02, -5.4665e-02,\n",
      "          6.7937e-02, -1.7461e-02,  1.7078e-02,  1.0808e-01, -2.6496e-02,\n",
      "          2.1596e-02,  4.9162e-02,  7.5676e-02, -2.7359e-03,  1.5555e-03,\n",
      "         -1.3134e-02, -9.9415e-02, -9.1044e-02, -5.1607e-02, -6.5626e-02,\n",
      "          5.8593e-02,  1.3012e-01,  6.5895e-02, -1.6070e-01, -6.9348e-02],\n",
      "        [ 4.6139e-02,  4.8386e-02,  1.1833e-01,  3.7313e-03, -2.4984e-03,\n",
      "          1.3036e-01, -5.3751e-02, -5.7430e-02,  5.0562e-02, -1.4937e-01,\n",
      "         -8.0468e-02,  7.9236e-02,  1.9756e-02, -2.5375e-01,  3.6503e-02,\n",
      "         -1.1842e-01,  1.8931e-01, -9.6139e-03, -6.8103e-02,  3.9115e-03,\n",
      "         -2.5747e-02,  2.5792e-02, -2.2910e-01,  1.7038e-02, -1.6164e-01,\n",
      "          4.2532e-02, -5.9440e-02,  1.3031e-01, -9.8862e-02,  9.9510e-02,\n",
      "          1.7237e-01,  5.1022e-02,  3.8031e-02, -1.3633e-01,  1.3902e-02,\n",
      "         -1.5859e-01, -1.8753e-01, -4.5612e-02, -1.6102e-02, -8.1184e-02,\n",
      "         -1.0050e-01, -9.3775e-02,  6.2148e-02, -3.8511e-02,  6.7854e-02,\n",
      "         -6.1651e-02,  7.6403e-02, -4.2355e-02,  3.7163e-02,  3.2477e-02,\n",
      "          1.7456e-01, -1.3854e-01,  1.0107e-01, -3.6025e-02, -5.0270e-02,\n",
      "         -4.9833e-02,  9.4031e-02,  1.2664e-01, -1.2390e-01, -5.3340e-02,\n",
      "          9.4605e-03, -3.1202e-02, -1.1896e-01,  5.3635e-02, -3.3237e-02,\n",
      "          9.7185e-02, -6.7839e-02,  4.3341e-02, -1.3396e-01,  1.2555e-01,\n",
      "         -1.3162e-01,  1.3631e-01, -6.0980e-02, -3.8906e-02, -5.1475e-02,\n",
      "         -5.1846e-02,  3.6744e-02,  8.5895e-02, -1.1916e-02,  6.9675e-02,\n",
      "          9.4564e-02,  7.8481e-02, -8.4512e-03, -9.1886e-02,  1.8164e-01,\n",
      "          5.5575e-02, -3.9694e-02, -6.0185e-02, -5.9118e-02, -2.8571e-02,\n",
      "         -1.0922e-01,  1.2306e-01,  1.4388e-01,  1.5142e-01,  2.5840e-02,\n",
      "          3.9948e-02,  4.7793e-02, -7.5297e-03,  7.2879e-02, -2.8157e-02],\n",
      "        [ 5.8619e-03, -2.9592e-02,  1.2382e-01,  1.1311e-01,  3.2563e-02,\n",
      "         -1.8380e-02, -6.2570e-02, -1.5059e-01, -1.5144e-01,  4.6089e-02,\n",
      "          1.5036e-01,  1.3389e-02,  1.1727e-01, -1.4015e-02, -6.9529e-03,\n",
      "         -4.2102e-02, -9.9777e-02, -1.6161e-01, -4.6569e-03, -6.3583e-02,\n",
      "         -4.0051e-02,  1.1262e-02, -1.9384e-02,  3.4803e-02, -1.2914e-01,\n",
      "         -9.2462e-02, -1.4438e-02,  1.0605e-02, -9.4727e-02,  2.3813e-02,\n",
      "          8.3207e-02, -1.7868e-01,  9.0408e-02,  9.1954e-02,  2.6292e-03,\n",
      "          3.2967e-02, -5.6250e-03,  1.5070e-01,  5.5888e-02, -1.1127e-01,\n",
      "          7.4941e-02, -1.0542e-02, -7.4798e-02,  1.6976e-01,  8.4763e-02,\n",
      "          1.2002e-01, -1.9516e-01, -3.3234e-02, -8.7709e-02, -2.0177e-02,\n",
      "          1.1630e-01, -1.5017e-01,  3.8105e-02, -7.0408e-02,  6.4307e-02,\n",
      "          4.9432e-02, -1.4510e-01, -8.8255e-02, -1.0962e-01,  1.1150e-03,\n",
      "          6.1838e-02, -1.6037e-02, -5.5276e-02, -3.3127e-02,  1.2328e-01,\n",
      "          1.2723e-01, -1.3472e-03, -6.4997e-02,  6.3297e-02,  1.5371e-02,\n",
      "          4.1038e-02,  6.5661e-04, -1.2214e-01, -6.7034e-02,  1.4003e-01,\n",
      "         -6.3167e-02, -4.5011e-02, -1.0583e-01, -9.5111e-02, -9.2470e-03,\n",
      "         -4.5035e-03, -4.0364e-02,  5.9513e-02, -1.2900e-02, -4.5861e-02,\n",
      "         -3.1450e-02, -1.4678e-02,  1.3456e-02, -4.9770e-02,  1.1942e-01,\n",
      "          1.5273e-01,  1.1697e-01,  1.4795e-04,  7.6600e-02,  1.7014e-01,\n",
      "          2.8136e-02, -7.4380e-02, -8.1507e-02,  4.2966e-03,  1.3087e-02],\n",
      "        [-1.0109e-01,  7.5017e-02,  4.3046e-02,  5.1599e-02,  1.0920e-02,\n",
      "          4.4368e-02, -1.8400e-01, -5.2990e-02,  3.1205e-02, -3.0953e-02,\n",
      "          6.6866e-02, -9.6002e-02,  4.1691e-02, -5.2673e-02,  4.9242e-02,\n",
      "         -1.2523e-01,  1.1798e-03,  1.4533e-01, -1.8317e-02, -1.0363e-01,\n",
      "         -9.2024e-02,  1.8939e-03,  1.2947e-02, -1.5470e-01,  2.7941e-01,\n",
      "         -1.5882e-01, -1.3558e-01, -1.4602e-01, -5.4613e-02,  6.9642e-02,\n",
      "          1.0707e-01,  1.8789e-01, -8.5992e-02,  1.2585e-02, -1.1778e-01,\n",
      "         -5.5936e-02, -8.4257e-02, -1.7084e-01, -1.7201e-02,  6.9034e-02,\n",
      "          9.5211e-02, -4.0151e-02,  3.7071e-02, -5.0469e-02,  1.0742e-01,\n",
      "          1.1088e-02,  1.1056e-01,  2.0795e-01,  1.7000e-01,  1.1174e-02,\n",
      "         -3.3107e-03,  6.3586e-02,  3.7445e-02, -1.6734e-01,  1.0413e-01,\n",
      "          1.2414e-01, -6.7949e-02,  5.2606e-02, -8.6310e-02, -1.1839e-01,\n",
      "          1.2850e-01,  1.7428e-01, -1.1043e-01, -9.0863e-02, -1.8544e-01,\n",
      "          1.3488e-02,  1.4681e-03, -1.0873e-01,  4.9794e-02, -1.3614e-02,\n",
      "          1.1154e-01, -9.6140e-03, -1.1654e-01, -1.0798e-01, -8.4481e-02,\n",
      "         -1.0632e-01,  6.2518e-02,  6.8292e-02, -2.8648e-02, -6.3336e-02,\n",
      "          1.3703e-02,  8.1076e-02,  8.3332e-02,  1.3062e-01,  1.3808e-01,\n",
      "         -9.8931e-02, -6.4946e-02,  1.3704e-02,  5.5133e-02, -9.0411e-02,\n",
      "         -2.8019e-02, -4.4603e-02,  3.9825e-02, -5.6863e-02,  7.9439e-02,\n",
      "         -3.4567e-02,  9.5617e-02, -2.6475e-02, -1.1721e-02,  3.9605e-02],\n",
      "        [ 4.8469e-02, -2.5577e-02, -7.9721e-02, -2.8891e-02,  7.7087e-02,\n",
      "          6.4823e-02, -2.1993e-02,  9.5295e-02, -6.8029e-02,  4.7245e-02,\n",
      "          3.4338e-03,  1.0418e-02,  1.5196e-02,  1.0659e-01,  4.0379e-02,\n",
      "         -1.2577e-01, -3.6373e-02,  2.2826e-02, -7.6232e-02, -8.2000e-02,\n",
      "         -6.3742e-02,  3.7724e-02,  2.9620e-02,  2.0294e-01,  1.8291e-01,\n",
      "         -9.2482e-02, -5.7905e-02, -1.5378e-01, -7.1452e-02,  1.3578e-01,\n",
      "         -2.4989e-02, -1.0715e-01, -1.3295e-01,  3.7320e-02,  7.0199e-02,\n",
      "         -1.2955e-01,  8.2333e-02,  1.7611e-01, -8.2820e-02,  4.4780e-02,\n",
      "         -1.6919e-02, -9.3568e-03, -9.8624e-02,  3.0866e-02,  1.0481e-01,\n",
      "         -7.7548e-02,  1.3058e-01, -2.7428e-02, -9.7029e-03, -5.3560e-02,\n",
      "          6.9170e-02, -7.9305e-02, -1.6615e-01,  1.1657e-01, -8.0376e-02,\n",
      "          3.8842e-02, -1.2311e-01, -9.1159e-02,  9.8044e-02,  1.3744e-02,\n",
      "          1.0993e-01, -1.1652e-01, -3.2113e-02, -6.7464e-02,  9.7348e-02,\n",
      "         -1.0102e-01, -2.3247e-02,  5.3279e-02, -1.5293e-02, -6.1142e-02,\n",
      "          6.0921e-04, -6.5871e-02, -2.1590e-01,  2.3065e-02, -5.0690e-02,\n",
      "         -9.4237e-02, -1.1667e-02, -4.0916e-03,  1.7999e-02, -1.0959e-01,\n",
      "         -1.9098e-01, -7.3859e-03, -5.5193e-02, -3.9059e-02, -1.5879e-01,\n",
      "          2.4363e-02, -4.1607e-02,  1.7249e-02,  2.5374e-02,  3.3149e-03,\n",
      "          5.1456e-02,  3.3573e-02, -2.0948e-02, -2.1271e-02,  1.0318e-01,\n",
      "         -3.7960e-03, -3.2025e-02,  6.8646e-02, -1.7661e-02, -2.0849e-02],\n",
      "        [-8.5480e-02,  1.2414e-01, -1.3450e-01, -1.3371e-02, -1.0964e-01,\n",
      "         -2.1258e-01, -3.7893e-02,  3.4279e-02,  1.0983e-01,  1.2254e-01,\n",
      "          5.0913e-02, -1.5217e-01,  1.9594e-02, -1.0004e-01,  5.3628e-02,\n",
      "          1.3979e-01, -1.3990e-01,  1.9594e-03, -2.8928e-02,  5.3872e-02,\n",
      "          5.6536e-02, -9.3563e-02,  2.1384e-01, -1.3715e-01,  6.4513e-02,\n",
      "          1.4739e-02,  1.1957e-02, -2.0470e-02,  1.2002e-02,  3.0145e-02,\n",
      "         -2.0010e-02,  5.2535e-02,  1.7398e-01, -1.2527e-01, -6.3806e-02,\n",
      "          7.6547e-02,  7.4786e-02, -1.2163e-01, -7.1752e-02, -2.7321e-02,\n",
      "         -1.9048e-01,  3.4529e-03, -1.0171e-01,  6.7441e-02, -6.4311e-02,\n",
      "         -1.1417e-01,  1.1886e-01, -9.2200e-02, -8.6885e-02, -3.4438e-02,\n",
      "          1.6857e-01,  1.3888e-01,  5.6224e-02,  1.0006e-01,  1.8872e-02,\n",
      "          9.7852e-02, -7.6577e-02, -7.3054e-02, -1.7257e-02,  3.3742e-02,\n",
      "         -8.6660e-02, -1.3592e-02, -5.6244e-02,  5.6168e-02, -1.3168e-01,\n",
      "         -1.8165e-02, -3.6484e-02, -3.6497e-02, -1.2689e-01, -5.8757e-02,\n",
      "         -9.3661e-02,  7.0829e-02, -6.5922e-02,  6.2270e-02,  1.3687e-01,\n",
      "          1.1372e-01, -1.3975e-01, -4.3145e-02, -3.5882e-03,  7.3685e-02,\n",
      "         -1.3741e-02,  3.2089e-02, -3.2839e-02,  9.8754e-02,  7.7407e-02,\n",
      "          3.5038e-02,  1.3101e-01,  1.0570e-01,  5.8440e-02,  9.7114e-02,\n",
      "          2.7495e-02,  4.6579e-02,  4.1320e-02, -2.0331e-02,  9.5416e-02,\n",
      "         -2.9892e-02, -1.7301e-01,  5.2914e-03,  1.2723e-02,  4.6767e-02]]))\n",
      "(None, tensor([-0.0144,  0.0008,  0.1614,  0.0895,  0.1812, -0.1016,  0.0203, -0.0114,\n",
      "         0.0526, -0.1286]))\n"
     ]
    }
   ],
   "source": [
    "# print model parameters\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "         print(p.name, p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

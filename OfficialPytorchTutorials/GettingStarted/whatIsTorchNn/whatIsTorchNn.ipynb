{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data from MNIST\n",
    "We download data from an online source(deeplearning.net) to local directory.\n",
    "The data is deserialized using pickle into tuples for train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), (x_test, y_test)) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape, x_train = (50000, 784), y_train = (50000,)\n"
     ]
    }
   ],
   "source": [
    "print (\"Training set shape, x_train = {}, y_train = {}\".format(x_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set shape, x_valid = (10000, 784), y_valid = (10000,)\n"
     ]
    }
   ],
   "source": [
    "print (\"Validation set shape, x_valid = {}, y_valid = {}\".format(x_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shape, x_test = (10000, 784), y_test = (10000,)\n"
     ]
    }
   ],
   "source": [
    "print (\"Test set shape, x_test = {}, y_test = {}\".format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display input with label\n",
    "In this section we write some utils to display input with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_example(x_train, y_train, N=0):\n",
    "    plt.imshow(x_train[N].reshape((28, 28)), cmap=\"gray\")\n",
    "    plt.title(\"digit label = {}\".format(y_train[N]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_example(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 50000 examples with 784 features each\n",
      "Training labels are in range [0, 9]\n"
     ]
    }
   ],
   "source": [
    "n, c = x_train.shape\n",
    "print(\"Training set has {} examples with {} features each\".format(n,c))\n",
    "print(\"Training labels are in range [{}, {}]\".format(y_train.min(), y_train.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN from scrath\n",
    "In this section, we implement a neural network without torch.nn\n",
    "using basic torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb, weights, bias):\n",
    "#    return log_softmax(xb @ weights + bias)\n",
    "    return log_softmax(xb.mm(weights) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ -9.6709, -14.7251,  -8.9294,  -1.4262, -15.9069,  -0.2758, -11.7755,\n",
      "         -7.6929,  -9.6340,  -9.9914], grad_fn=<SelectBackward>), torch.Size([64, 10]))\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "preds = model(xb, weights, bias)  # predictions\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1890, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9219)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training on initial weights = 2.3849799633%%\n",
      "Accuracy on validation  on initial weights = 2.38120532036%%\n",
      "Accuracy on training after epoch#0, loss = 0.324643403292, acc = 90.6359939575%%\n",
      "Accuracy on validation after epoch#0, loss = 0.306144207716, acc = 91.3399963379%%\n",
      "Accuracy on training after epoch#1, loss = 0.305484950542, acc = 91.2080001831%%\n",
      "Accuracy on validation after epoch#1, loss = 0.292917281389, acc = 91.8299942017%%\n",
      "Accuracy on training after epoch#2, loss = 0.296874523163, acc = 91.5120010376%%\n",
      "Accuracy on validation after epoch#2, loss = 0.288196325302, acc = 91.9199981689%%\n",
      "Accuracy on training after epoch#3, loss = 0.291536957026, acc = 91.6459960938%%\n",
      "Accuracy on validation after epoch#3, loss = 0.2859634161, acc = 91.9499969482%%\n",
      "Accuracy on training after epoch#4, loss = 0.287716329098, acc = 91.7740020752%%\n",
      "Accuracy on validation after epoch#4, loss = 0.284799307585, acc = 92.0199966431%%\n"
     ]
    }
   ],
   "source": [
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print (\"Accuracy on training on initial weights = {}%%\".\n",
    "           format(loss_func(model(x_train, weights, bias), y_train), 100*accuracy(model(x_train, weights, bias), y_train)))\n",
    "    print (\"Accuracy on validation  on initial weights = {}%%\".\n",
    "           format( loss_func(model(x_valid, weights, bias), y_valid), 100*accuracy(model(x_valid, weights, bias), y_valid)))\n",
    "\n",
    "lr = 0.5\n",
    "bs = 64\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    for i in range (n/bs):\n",
    "        xb = x_train[i*bs:min(n, (i+1)*bs)]  # a mini-batch from x\n",
    "        yb = y_train[i*bs:min(n, (i+1)*bs)]\n",
    "        preds = model(xb, weights, bias)  # predictions\n",
    "        loss = loss_func(preds, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print (\"Accuracy on training after epoch#{}, loss = {}, acc = {}%%\".\n",
    "               format(e, loss_func(model(x_train, weights, bias), y_train), 100*accuracy(model(x_train, weights, bias), y_train)))\n",
    "        print (\"Accuracy on validation after epoch#{}, loss = {}, acc = {}%%\".\n",
    "               format(e, loss_func(model(x_valid, weights, bias), y_valid), 100*accuracy(model(x_valid, weights, bias), y_valid)))\n",
    "    #    weights = weights - learning_rate * weights.grad\n",
    "    #    bias = bias - learning_rate * bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using torch.nn.functional\n",
    "In this section we refactor our above code and use functional package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy\n",
    "def forward(xb, weights, bias):\n",
    "    return xb.mm(weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.2848), tensor(0.9202))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(loss_func(forward(x_valid, weights, bias), y_valid), accuracy(forward(x_valid, weights, bias), y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using functional package\n",
    "We repeat same exercise using functional package. Try to implement everything on your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(2.3528), tensor(0.0957))\n",
      "Accuracy on training on initial weights = 0.287716329098%%\n",
      "Accuracy on validation  on initial weights = 0.284799307585%%\n",
      "Accuracy on training after epoch#0, loss = 0.287716329098, acc = 91.7740020752%%\n",
      "Accuracy on validation after epoch#0, loss = 0.284799307585, acc = 92.0199966431%%\n",
      "Accuracy on training after epoch#1, loss = 0.287716329098, acc = 91.7740020752%%\n",
      "Accuracy on validation after epoch#1, loss = 0.284799307585, acc = 92.0199966431%%\n",
      "Accuracy on training after epoch#2, loss = 0.287716329098, acc = 91.7740020752%%\n",
      "Accuracy on validation after epoch#2, loss = 0.284799307585, acc = 92.0199966431%%\n",
      "Accuracy on training after epoch#3, loss = 0.287716329098, acc = 91.7740020752%%\n",
      "Accuracy on validation after epoch#3, loss = 0.284799307585, acc = 92.0199966431%%\n",
      "Accuracy on training after epoch#4, loss = 0.287716329098, acc = 91.7740020752%%\n",
      "Accuracy on validation after epoch#4, loss = 0.284799307585, acc = 92.0199966431%%\n",
      "Accuracy on training after epoch#5, loss = 0.287716329098, acc = 91.7740020752%%\n",
      "Accuracy on validation after epoch#5, loss = 0.284799307585, acc = 92.0199966431%%\n",
      "Accuracy on training after epoch#6, loss = 0.287716329098, acc = 91.7740020752%%\n",
      "Accuracy on validation after epoch#6, loss = 0.284799307585, acc = 92.0199966431%%\n",
      "Accuracy on training after epoch#7, loss = 0.287716329098, acc = 91.7740020752%%\n",
      "Accuracy on validation after epoch#7, loss = 0.284799307585, acc = 92.0199966431%%\n",
      "Accuracy on training after epoch#8, loss = 0.287716329098, acc = 91.7740020752%%\n",
      "Accuracy on validation after epoch#8, loss = 0.284799307585, acc = 92.0199966431%%\n",
      "Accuracy on training after epoch#9, loss = 0.287716329098, acc = 91.7740020752%%\n",
      "Accuracy on validation after epoch#9, loss = 0.284799307585, acc = 92.0199966431%%\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(784, 10) / math.sqrt(784.0)\n",
    "w.requires_grad_()\n",
    "b = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(loss_func(forward(x_valid, w, b), y_valid), accuracy(forward(x_valid, w, b), y_valid))\n",
    "\n",
    "with torch.no_grad():\n",
    "    print (\"Accuracy on training on initial weights = {}%%\".\n",
    "           format(loss_func(model(x_train, weights, bias), y_train), 100*accuracy(model(x_train, weights, bias), y_train)))\n",
    "    print (\"Accuracy on validation  on initial weights = {}%%\".\n",
    "           format( loss_func(model(x_valid, weights, bias), y_valid), 100*accuracy(model(x_valid, weights, bias), y_valid)))\n",
    "\n",
    "    \n",
    "lr = 0.5\n",
    "epochs = 10\n",
    "bs = 64\n",
    "for e in range(epochs):\n",
    "    for i in range(n/bs):\n",
    "        b_start = i*bs\n",
    "        b_end = min(n, (i+1)*bs)\n",
    "        xb = x_train[b_start:b_end]\n",
    "        yb = y_train[b_start:b_end]\n",
    "        pred = forward(xb, w, b)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            w -= w.grad*lr\n",
    "            b -= b.grad*lr\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print (\"Accuracy on training after epoch#{}, loss = {}, acc = {}%%\".\n",
    "               format(e, loss_func(model(x_train, weights, bias), y_train), 100*accuracy(model(x_train, weights, bias), y_train)))\n",
    "        print (\"Accuracy on validation after epoch#{}, loss = {}, acc = {}%%\".\n",
    "               format(e, loss_func(model(x_valid, weights, bias), y_valid), 100*accuracy(model(x_valid, weights, bias), y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use nn module\n",
    "Now we use nn.module to encapsulate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class MnistLogistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistLogistic, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return xb.mm(self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, x_train, y_train, x_valid, y_valid, bs = 64, lr = 0.5):\n",
    "    n, num_feats = x_train.shape\n",
    "    with torch.no_grad():\n",
    "        print (\"Accuracy on training on initial weights = {}%%\".\n",
    "               format(loss_func(model(x_train), y_train), 100*accuracy(model(x_train), y_train)))\n",
    "        print (\"Accuracy on validation  on initial weights = {}%%\".\n",
    "               format( loss_func(model(x_valid), y_valid), 100*accuracy(model(x_valid), y_valid)))\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(n/bs):\n",
    "            b_start = i*bs\n",
    "            b_end = min(n, (i+1)*bs)\n",
    "            xb = x_train[b_start:b_end]\n",
    "            yb = y_train[b_start:b_end]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            print (\"Accuracy on training after epoch#{}, loss = {}, acc = {}%%\".\n",
    "                   format(e, loss_func(model(x_train), y_train), 100*accuracy(model(x_train), y_train)))\n",
    "            print (\"Accuracy on validation after epoch#{}, loss = {}, acc = {}%%\".\n",
    "                   format(e, loss_func(model(x_valid), y_valid), 100*accuracy(model(x_valid), y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training on initial weights = 2.40425419807%%\n",
      "Accuracy on validation  on initial weights = 2.40479540825%%\n",
      "Accuracy on training after epoch#0, loss = 0.325196832418, acc = 90.6240005493%%\n",
      "Accuracy on validation after epoch#0, loss = 0.306496471167, acc = 91.3099975586%%\n",
      "Accuracy on training after epoch#1, loss = 0.305829912424, acc = 91.2140045166%%\n",
      "Accuracy on validation after epoch#1, loss = 0.293144583702, acc = 91.8199996948%%\n",
      "Accuracy on training after epoch#2, loss = 0.297110497952, acc = 91.513999939%%\n",
      "Accuracy on validation after epoch#2, loss = 0.288365244865, acc = 91.9599990845%%\n",
      "Accuracy on training after epoch#3, loss = 0.291716247797, acc = 91.6580047607%%\n",
      "Accuracy on validation after epoch#3, loss = 0.286115825176, acc = 92.0199966431%%\n",
      "Accuracy on training after epoch#4, loss = 0.287865579128, acc = 91.7480010986%%\n",
      "Accuracy on validation after epoch#4, loss = 0.284949570894, acc = 92.0699996948%%\n",
      "Accuracy on training after epoch#5, loss = 0.284887582064, acc = 91.8180007935%%\n",
      "Accuracy on validation after epoch#5, loss = 0.284342139959, acc = 92.0899963379%%\n",
      "Accuracy on training after epoch#6, loss = 0.282467544079, acc = 91.8939971924%%\n",
      "Accuracy on validation after epoch#6, loss = 0.284055382013, acc = 92.1399993896%%\n",
      "Accuracy on training after epoch#7, loss = 0.280436575413, acc = 91.9720001221%%\n",
      "Accuracy on validation after epoch#7, loss = 0.283970445395, acc = 92.1299972534%%\n",
      "Accuracy on training after epoch#8, loss = 0.278686314821, acc = 91.9899978638%%\n",
      "Accuracy on validation after epoch#8, loss = 0.284017205238, acc = 92.1100006104%%\n",
      "Accuracy on training after epoch#9, loss = 0.277150541544, acc = 92.0459976196%%\n",
      "Accuracy on validation after epoch#9, loss = 0.284150838852, acc = 92.1299972534%%\n"
     ]
    }
   ],
   "source": [
    "model = MnistLogistic()\n",
    "fit(model, x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class MnistLogisticB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistLogisticB, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 10)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.fc1(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training on initial weights = 0.277150541544%%\n",
      "Accuracy on validation  on initial weights = 0.284150838852%%\n",
      "Accuracy on training after epoch#0, loss = 0.275786787271, acc = 92.0780029297%%\n",
      "Accuracy on validation after epoch#0, loss = 0.284346610308, acc = 92.1199951172%%\n",
      "Accuracy on training after epoch#1, loss = 0.274560540915, acc = 92.1039962769%%\n",
      "Accuracy on validation after epoch#1, loss = 0.284584701061, acc = 92.1500015259%%\n",
      "Accuracy on training after epoch#2, loss = 0.273445546627, acc = 92.1299972534%%\n",
      "Accuracy on validation after epoch#2, loss = 0.284852355719, acc = 92.1399993896%%\n",
      "Accuracy on training after epoch#3, loss = 0.272421598434, acc = 92.1500015259%%\n",
      "Accuracy on validation after epoch#3, loss = 0.285141736269, acc = 92.1399993896%%\n",
      "Accuracy on training after epoch#4, loss = 0.27148270607, acc = 92.1760025024%%\n",
      "Accuracy on validation after epoch#4, loss = 0.285443007946, acc = 92.0999984741%%\n",
      "Accuracy on training after epoch#5, loss = 0.270608127117, acc = 92.1959991455%%\n",
      "Accuracy on validation after epoch#5, loss = 0.285754978657, acc = 92.0999984741%%\n",
      "Accuracy on training after epoch#6, loss = 0.269795566797, acc = 92.2259979248%%\n",
      "Accuracy on validation after epoch#6, loss = 0.286072701216, acc = 92.0899963379%%\n",
      "Accuracy on training after epoch#7, loss = 0.269034206867, acc = 92.2420043945%%\n",
      "Accuracy on validation after epoch#7, loss = 0.286393582821, acc = 92.0799942017%%\n",
      "Accuracy on training after epoch#8, loss = 0.268317252398, acc = 92.2799987793%%\n",
      "Accuracy on validation after epoch#8, loss = 0.286715000868, acc = 92.0699996948%%\n",
      "Accuracy on training after epoch#9, loss = 0.267643451691, acc = 92.2880020142%%\n",
      "Accuracy on validation after epoch#9, loss = 0.287036120892, acc = 92.0699996948%%\n"
     ]
    }
   ],
   "source": [
    "modelB = MnistLogisticB()\n",
    "fit(model, x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use optim module\n",
    "Now we make use of Adam optimizer from optim module and use it instead of manually updaing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelB2 = MnistLogisticB()\n",
    "optimizer = optim.SGD(modelB2.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitB(model, opt, x_train, y_train, x_valid, y_valid, bs = 64):\n",
    "    n, num_feats = x_train.shape\n",
    "    with torch.no_grad():\n",
    "        print (\"Accuracy on training on initial weights = {}%%\".\n",
    "               format(loss_func(model(x_train), y_train), 100*accuracy(model(x_train), y_train)))\n",
    "        print (\"Accuracy on validation  on initial weights = {}%%\".\n",
    "               format( loss_func(model(x_valid), y_valid), 100*accuracy(model(x_valid), y_valid)))\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(n/bs):\n",
    "            b_start = i*bs\n",
    "            b_end = min(n, (i+1)*bs)\n",
    "            xb = x_train[b_start:b_end]\n",
    "            yb = y_train[b_start:b_end]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            print (\"Accuracy on training after epoch#{}, loss = {}, acc = {}%%\".\n",
    "                   format(e, loss_func(model(x_train), y_train), 100*accuracy(model(x_train), y_train)))\n",
    "            print (\"Accuracy on validation after epoch#{}, loss = {}, acc = {}%%\".\n",
    "                   format(e, loss_func(model(x_valid), y_valid), 100*accuracy(model(x_valid), y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training on initial weights = 2.3481528759%%\n",
      "Accuracy on validation  on initial weights = 2.35114264488%%\n",
      "Accuracy on training after epoch#0, loss = 0.370350301266, acc = 89.641998291%%\n",
      "Accuracy on validation after epoch#0, loss = 0.339498013258, acc = 90.7799987793%%\n",
      "Accuracy on training after epoch#1, loss = 0.331948369741, acc = 90.6699981689%%\n",
      "Accuracy on validation after epoch#1, loss = 0.308653444052, acc = 91.5299987793%%\n",
      "Accuracy on training after epoch#2, loss = 0.314577966928, acc = 91.1580047607%%\n",
      "Accuracy on validation after epoch#2, loss = 0.295205026865, acc = 91.7599945068%%\n",
      "Accuracy on training after epoch#3, loss = 0.304036706686, acc = 91.439994812%%\n",
      "Accuracy on validation after epoch#3, loss = 0.287277877331, acc = 91.9199981689%%\n",
      "Accuracy on training after epoch#4, loss = 0.296733736992, acc = 91.6980056763%%\n",
      "Accuracy on validation after epoch#4, loss = 0.281948834658, acc = 92.0799942017%%\n",
      "Accuracy on training after epoch#5, loss = 0.291266262531, acc = 91.8799972534%%\n",
      "Accuracy on validation after epoch#5, loss = 0.278084486723, acc = 92.1699981689%%\n",
      "Accuracy on training after epoch#6, loss = 0.286958813667, acc = 92.0%%\n",
      "Accuracy on validation after epoch#6, loss = 0.275142818689, acc = 92.2200012207%%\n",
      "Accuracy on training after epoch#7, loss = 0.283442944288, acc = 92.1100006104%%\n",
      "Accuracy on validation after epoch#7, loss = 0.27282038331, acc = 92.2399978638%%\n",
      "Accuracy on training after epoch#8, loss = 0.280491262674, acc = 92.1959991455%%\n",
      "Accuracy on validation after epoch#8, loss = 0.270941197872, acc = 92.3600006104%%\n",
      "Accuracy on training after epoch#9, loss = 0.277958869934, acc = 92.2819976807%%\n",
      "Accuracy on validation after epoch#9, loss = 0.269389122725, acc = 92.4499969482%%\n"
     ]
    }
   ],
   "source": [
    "fitB(modelB2, optimizer, x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dataset\n",
    "Now we implement a dataset to provide an iterator on mini-batches of mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "modelB3 = MnistLogisticB()\n",
    "optimizer = optim.SGD(modelB3.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitC(model, opt, train_ds, valid_ds, bs = 64):\n",
    "    n, num_feats = x_train.shape\n",
    "    with torch.no_grad():\n",
    "        print (\"Accuracy on training on initial weights = {}%%\".\n",
    "               format(loss_func(model(x_train), y_train), 100*accuracy(model(x_train), y_train)))\n",
    "        print (\"Accuracy on validation  on initial weights = {}%%\".\n",
    "               format( loss_func(model(x_valid), y_valid), 100*accuracy(model(x_valid), y_valid)))\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(n/bs):\n",
    "            b_start = i*bs\n",
    "            b_end = min(n, (i+1)*bs)\n",
    "            xb, yb = train_ds[b_start:b_end]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            print (\"Accuracy on training after epoch#{}, loss = {}, acc = {}%%\".\n",
    "                   format(e, loss_func(model(x_train), y_train), 100*accuracy(model(x_train), y_train)))\n",
    "            print (\"Accuracy on validation after epoch#{}, loss = {}, acc = {}%%\".\n",
    "                   format(e, loss_func(model(x_valid), y_valid), 100*accuracy(model(x_valid), y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training on initial weights = 2.30613493919%%\n",
      "Accuracy on validation  on initial weights = 2.30903148651%%\n",
      "Accuracy on training after epoch#0, loss = 0.370242625475, acc = 89.5859985352%%\n",
      "Accuracy on validation after epoch#0, loss = 0.339536756277, acc = 90.8399963379%%\n",
      "Accuracy on training after epoch#1, loss = 0.331855386496, acc = 90.6979980469%%\n",
      "Accuracy on validation after epoch#1, loss = 0.308734446764, acc = 91.5499954224%%\n",
      "Accuracy on training after epoch#2, loss = 0.314504891634, acc = 91.1660003662%%\n",
      "Accuracy on validation after epoch#2, loss = 0.295303970575, acc = 91.7299957275%%\n",
      "Accuracy on training after epoch#3, loss = 0.303981512785, acc = 91.4759979248%%\n",
      "Accuracy on validation after epoch#3, loss = 0.287387877703, acc = 91.9499969482%%\n",
      "Accuracy on training after epoch#4, loss = 0.296696424484, acc = 91.7119979858%%\n",
      "Accuracy on validation after epoch#4, loss = 0.282062709332, acc = 92.0799942017%%\n",
      "Accuracy on training after epoch#5, loss = 0.29124084115, acc = 91.8759994507%%\n",
      "Accuracy on validation after epoch#5, loss = 0.278200238943, acc = 92.1399993896%%\n",
      "Accuracy on training after epoch#6, loss = 0.286945223808, acc = 92.0%%\n",
      "Accuracy on validation after epoch#6, loss = 0.275255918503, acc = 92.2299957275%%\n",
      "Accuracy on training after epoch#7, loss = 0.283435225487, acc = 92.09400177%%\n",
      "Accuracy on validation after epoch#7, loss = 0.272932291031, acc = 92.2900009155%%\n",
      "Accuracy on training after epoch#8, loss = 0.280486017466, acc = 92.1699981689%%\n",
      "Accuracy on validation after epoch#8, loss = 0.271049827337, acc = 92.3899993896%%\n",
      "Accuracy on training after epoch#9, loss = 0.27796214819, acc = 92.2420043945%%\n",
      "Accuracy on validation after epoch#9, loss = 0.269495308399, acc = 92.3799972534%%\n"
     ]
    }
   ],
   "source": [
    "fitC(modelB3, optimizer, train_ds, valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using dataLoader\n",
    "Now we move from dataset to dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitC(model, opt, train_dl, valid_ds, bs = 64):\n",
    "    n, num_feats = x_train.shape\n",
    "    with torch.no_grad():\n",
    "        print (\"Accuracy on training on initial weights = {}%%\".\n",
    "               format(loss_func(model(x_train), y_train), 100*accuracy(model(x_train), y_train)))\n",
    "        print (\"Accuracy on validation  on initial weights = {}%%\".\n",
    "               format( loss_func(model(x_valid), y_valid), 100*accuracy(model(x_valid), y_valid)))\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            print (\"Accuracy on training after epoch#{}, loss = {}, acc = {}%%\".\n",
    "                   format(e, loss_func(model(x_train), y_train), 100*accuracy(model(x_train), y_train)))\n",
    "            print (\"Accuracy on validation after epoch#{}, loss = {}, acc = {}%%\".\n",
    "                   format(e, loss_func(model(x_valid), y_valid), 100*accuracy(model(x_valid), y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelB4 = MnistLogisticB()\n",
    "optimizer = optim.SGD(modelB4.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training on initial weights = 2.31860256195%%\n",
      "Accuracy on validation  on initial weights = 2.31980133057%%\n",
      "Accuracy on training after epoch#0, loss = 0.373152822256, acc = 89.5219955444%%\n",
      "Accuracy on validation after epoch#0, loss = 0.340128093958, acc = 90.7900009155%%\n",
      "Accuracy on training after epoch#1, loss = 0.333486914635, acc = 90.6159973145%%\n",
      "Accuracy on validation after epoch#1, loss = 0.308661967516, acc = 91.6100006104%%\n",
      "Accuracy on training after epoch#2, loss = 0.315741688013, acc = 91.1399993896%%\n",
      "Accuracy on validation after epoch#2, loss = 0.295146524906, acc = 91.8299942017%%\n",
      "Accuracy on training after epoch#3, loss = 0.305053055286, acc = 91.4720001221%%\n",
      "Accuracy on validation after epoch#3, loss = 0.287260949612, acc = 91.9899978638%%\n",
      "Accuracy on training after epoch#4, loss = 0.297685474157, acc = 91.702003479%%\n",
      "Accuracy on validation after epoch#4, loss = 0.281992733479, acc = 92.0599975586%%\n",
      "Accuracy on training after epoch#5, loss = 0.292188853025, acc = 91.8099975586%%\n",
      "Accuracy on validation after epoch#5, loss = 0.278187841177, acc = 92.1800003052%%\n",
      "Accuracy on training after epoch#6, loss = 0.287865579128, acc = 91.9540023804%%\n",
      "Accuracy on validation after epoch#6, loss = 0.275296956301, acc = 92.2299957275%%\n",
      "Accuracy on training after epoch#7, loss = 0.284336656332, acc = 92.033996582%%\n",
      "Accuracy on validation after epoch#7, loss = 0.273017674685, acc = 92.3199996948%%\n",
      "Accuracy on training after epoch#8, loss = 0.281380027533, acc = 92.1459960938%%\n",
      "Accuracy on validation after epoch#8, loss = 0.271177142859, acc = 92.3699951172%%\n",
      "Accuracy on training after epoch#9, loss = 0.278845310211, acc = 92.2379989624%%\n",
      "Accuracy on validation after epoch#9, loss = 0.269655704498, acc = 92.4099960327%%\n"
     ]
    }
   ],
   "source": [
    "fitC(modelB4, optimizer, train_dl, valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2079, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(modelB4(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final concise model training code\n",
    "Now we have a few simple for loops using iterable dataloaders, optimizer and a train/validation dataset.\n",
    "The code is very small and boilderplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    return loss.sum(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitModel(model, optimizer, train_dl, valid_dl, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, F.cross_entropy, xb, yb, optimizer)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(*[loss_batch(model, F.cross_entropy, xb, yb) for xb, yb in valid_dl])\n",
    "            val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelB5 = MnistLogisticB()\n",
    "optimizer = optim.SGD(modelB5.parameters(), lr=0.1)\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.34033641529083253)\n",
      "(1, 0.3088694522857666)\n",
      "(2, 0.29537068300247193)\n",
      "(3, 0.28749094414710996)\n",
      "(4, 0.2822225090503693)\n",
      "(5, 0.27841439590454103)\n",
      "(6, 0.2755183416366577)\n",
      "(7, 0.27323586492538454)\n",
      "(8, 0.27138883476257325)\n",
      "(9, 0.26986349115371705)\n",
      "(10, 0.26858336753845213)\n",
      "(11, 0.26749494862556455)\n",
      "(12, 0.2665594666481018)\n",
      "(13, 0.26574814367294314)\n",
      "(14, 0.2650391053199768)\n",
      "(15, 0.26441534366607666)\n",
      "(16, 0.263863587808609)\n",
      "(17, 0.26337311162948607)\n",
      "(18, 0.26293529777526853)\n",
      "(19, 0.26254311027526855)\n"
     ]
    }
   ],
   "source": [
    "fitModel(modelB5, optimizer, train_dl, valid_dl, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change to a convolutional neural network\n",
    "So far we have been using a simple FFNN for our training on MNIST.\n",
    "Now we use a convolutional neural network for the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTCnnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTCnnModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1.5300308917999268)\n",
      "(1, 1.0817527778625489)\n",
      "(2, 0.7931236612319946)\n",
      "(3, 0.7176864025115967)\n",
      "(4, 0.6169787571907044)\n",
      "(5, 0.5826909749984741)\n",
      "(6, 0.5240694478988648)\n",
      "(7, 0.4413734820365906)\n",
      "(8, 0.3488052159309387)\n",
      "(9, 0.3560064856529236)\n",
      "(10, 0.3631133273124695)\n",
      "(11, 0.35982017002105715)\n",
      "(12, 0.31815854501724244)\n",
      "(13, 0.29408917407989504)\n",
      "(14, 0.2923297614097595)\n",
      "(15, 0.28027480754852296)\n",
      "(16, 0.27282418813705445)\n",
      "(17, 0.26235407552719114)\n",
      "(18, 0.24567805786132813)\n",
      "(19, 0.2426101110458374)\n"
     ]
    }
   ],
   "source": [
    "cnnModel = MNISTCnnModel()\n",
    "optimizer = optim.SGD(cnnModel.parameters(), lr=0.1)\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs)\n",
    "\n",
    "fitModel(cnnModel, optimizer, train_dl, valid_dl, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.23126391363143922)\n",
      "(1, 0.22287462797164917)\n",
      "(2, 0.2140595742225647)\n",
      "(3, 0.20798238687515258)\n",
      "(4, 0.20223542881011963)\n",
      "(5, 0.19704772806167603)\n",
      "(6, 0.19249215593338012)\n",
      "(7, 0.1886595329284668)\n",
      "(8, 0.1839370771408081)\n",
      "(9, 0.18066054000854492)\n",
      "(10, 0.17819957342147827)\n",
      "(11, 0.1769318013191223)\n",
      "(12, 0.1747707197189331)\n",
      "(13, 0.173764244556427)\n",
      "(14, 0.17264201068878174)\n",
      "(15, 0.17211034574508666)\n",
      "(16, 0.17050830821990967)\n",
      "(17, 0.16735441493988037)\n",
      "(18, 0.16500684146881103)\n",
      "(19, 0.1633652871131897)\n"
     ]
    }
   ],
   "source": [
    "fitModel(cnnModel, optimizer, train_dl, valid_dl, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set is : 95.13%\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy on validation set is : %0.2f%%\" % (100* accuracy(cnnModel(x_valid), y_valid).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.1602298047065735)\n",
      "(1, 0.1580078477859497)\n",
      "(2, 0.15567488374710084)\n",
      "(3, 0.1544105640411377)\n",
      "(4, 0.15261572389602662)\n",
      "(5, 0.15069404430389405)\n",
      "(6, 0.14867582902908325)\n",
      "(7, 0.14723446893692016)\n",
      "(8, 0.14615018939971924)\n",
      "(9, 0.145003698348999)\n",
      "(10, 0.14369693126678468)\n",
      "(11, 0.1420660517692566)\n",
      "(12, 0.14021934061050415)\n",
      "(13, 0.13885206813812256)\n",
      "(14, 0.13716021308898926)\n",
      "(15, 0.13581849250793457)\n",
      "(16, 0.13432721633911132)\n",
      "(17, 0.13316046733856202)\n",
      "(18, 0.13259186248779298)\n",
      "(19, 0.13140884065628053)\n",
      "Accuracy on validation set is : 96.04%\n"
     ]
    }
   ],
   "source": [
    "fitModel(cnnModel, optimizer, train_dl, valid_dl, 20)\n",
    "print (\"Accuracy on validation set is : %0.2f%%\" % (100* accuracy(cnnModel(x_valid), y_valid).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
